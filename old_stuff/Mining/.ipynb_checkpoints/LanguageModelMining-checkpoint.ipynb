{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "nouns = []\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "for token in tokenizer.vocab.keys():\n",
    "    doc = nlp(token)\n",
    "    if doc[0].pos_ == \"NOUN\" or doc[0].pos_ == \"VERB\":\n",
    "        nouns.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertForMaskedLM.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = bert.bert.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = emb/emb.norm(dim = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, similar_tokens_matrix = torch.mm(emb, emb.transpose(0,1)).topk(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./test.txt', sep = '\\t', header = None)\n",
    "df.columns = ('rel', 'head','tail','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtLocation          479\n",
       "IsA                 396\n",
       "UsedFor             339\n",
       "CapableOf           280\n",
       "HasPrerequisite     226\n",
       "HasSubevent         170\n",
       "HasProperty         143\n",
       "HasA                 98\n",
       "Causes               93\n",
       "PartOf               52\n",
       "MadeOf               28\n",
       "ReceivesAction       22\n",
       "NotCapableOf         20\n",
       "Desires              17\n",
       "MotivatedByGoal      15\n",
       "CausesDesire          6\n",
       "CreatedBy             5\n",
       "NotHasProperty        4\n",
       "RelatedTo             3\n",
       "DefinedAs             2\n",
       "NotIsA                1\n",
       "HasFirstSubevent      1\n",
       "Name: rel, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_word(orig_w, new_w):\n",
    "    if \"#\" in new_w:\n",
    "        return False\n",
    "    if orig_w[:-1] in new_w:\n",
    "        return False\n",
    "    if any(char.isdigit() for char in new_w):\n",
    "        return False\n",
    "    return True\n",
    "relations = [\n",
    "    'AtLocation',\n",
    "    'IsA',            \n",
    "    'HasPrerequisite',     \n",
    "    'HasA',\n",
    "    'PartOf'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "with open('./candidate_BERT.txt', 'w') as f:\n",
    "    for n in nouns[115:135]:\n",
    "        token = tokenizer.convert_tokens_to_ids([n])[0]\n",
    "        similar_tokens = similar_tokens_matrix[token]\n",
    "        similar_words = tokenizer.convert_ids_to_tokens(similar_tokens.tolist())[5:15]\n",
    "        good_words = [w for w in similar_words if good_word(n, w)]\n",
    "        for idx, gw in enumerate(good_words):\n",
    "            if idx > 5:\n",
    "                continue\n",
    "            for r in relations:\n",
    "                count += 2\n",
    "                f.write('\\t'.join((r, n, gw, '2'))+'\\n')\n",
    "                f.write('\\t'.join((r, gw, n, '2'))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_triples = pd.read_csv('./candidate_BERT.txt', sep = '\\t', header= None, )\n",
    "len(df_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triples.columns = ('rel','head','tail','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./BERT_mining_pred_direct.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triples['prob'] =  df.prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df_triples.groupby(by=['head','tail'])\n",
    "df_filter = groups.apply(lambda g: g[g['prob'] == g['prob'].max()])\n",
    "df_filter[df_filter.prob > 0.9999].to_csv('./sorted_BERT_direct.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
